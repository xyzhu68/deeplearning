In this thesis we use neural netwok patching to deal with concept drift and transfer learning. As the data change, our patching method not only learns new features from the new data by training, it also identifies if the data can be predicted well by the original network. Data are diverted accordingly to the proper classifiers. Thus we can still benefit from the original network. The data are coming in stream and networks are trained continuously, which is easy to realize due to the architecture of deep neural networks. Experiments on engagement in different layers of neural network are executed. Different datasets are applied for our experiments. We engage into each network layer in samll dataset and engage into each block of network for relative big networks to find the best layer for engagement. We find that the patching algorithm used in this thesis can adapt changed data well and quickly. For convolution neural network, which is applied in this thesis, the engagement on the last layers can archieve better performance.



Deep neural networks are become popular in machine learning nowadays. The layered architecture makes it easy for continuous learning compared to traditional machine learning methods like decision tree, random forests, etc.. The hidden layers of the network represent features in different stage of abstraction. These layers can be attached and reused for modified or even new problems, namely for concept drift and transfer learning.
In the real world, the data are not always available immediately. Sometimes, data come in pieces and in a relative long time. The classifiers must therefore be trained continuously with the data currently available. The neural networks are suitable for such situations.
In this thesis the convolution neural networks (CNN) are used to implement the patching algorithm, which is an approach to deal with changed image inputs that represents concept drift and transfer learning. An original network is assumed to classify unchanged data very well. When changed data come, a special classifier is trained to predicate if the original network will misclassify. Data are then diverted to the patching network or original network accordingly. This way, the usually expensive original network can be reused and the whole network must not be trained from scratch.
Experiments on engagement in different layers of neural networks are also performed. Several datasets are applied for these experiments. We engage into each network layer in samll dataset and engage into each block of network for relative big networks to find the best layer for engagement.
We find that the patching algorithm used in this thesis can adapt changed data well and quickly. For convolution neural network, which is applied in this thesis, the engagement on the last layers can archieve better performance, since special features of changed data are necessary and import for the classification.



Conclusion
The CNN is very suitable for continous training on data stream, which are coming batchwise. The layered architecture of CNN makes it possible for engagement and reuse of pretrained models.
Our experiments show that number of filters of CNN can affect the performance of the patching network. But the relation is not linear. For simple data, too many filters do not leads to better performance. The more filters, the more resources are requested. So one must find the balance between performance and costs, especially for complex datasets.
Five data change types (flip, rotate, appear, remap and transfer) for datasets MNIST and NIST and three data change types (appear remap and transfer) for dataset Dog-Monkey are applied on the patching network. The experiments show that the patching algorithm can adapt the data change quickly and well, using metrics we defined (final accuracy, recovery speed, etc.). Besides the error detector, a model selector is also used to determine if the original network misclassifies. This model selector is not just trained with changed data. It compares the probability on correct prediction of base network and patching network. The model selector shows a better performance over the error detector in most cases.
Enagement is also done on datasets. For MNIST every convolution/maxpooling layer is engaged. For NIST every Conv-block is engaged. The patching network has a hidden layer: fully connected layer with 512 units. It shows that generally engagement on layers towards end of network can archieve better performance. Since the inner layers of CNN represents the general information and the last layers the special features, the last layers are necessary for extraction of new features of changed data.
Optimizations are conducted for classifiers on dataset NIST. It shows that dropout, normalization and the selection of optimizer and its learning rate can affect the performance of classifiers. The experiments show a relative greater improment on flip, rotate and appear, whose accuracy is lower before optimization. There are certainly many other possibilities to optimize the patching network, which can be done in the future work.
An interesting phenomenon observed in our experiments is the so called "fragile co-adaptation". Two layers are correlated. If an engagement is done between them, the correlation is destroyed. This leads to performance drop. An engagement at these positions shall be avoided.
For dataset Dog-Monkey, VGG16 and the technique "fine-tuning" is applied in the patching network. All Conv-blocks till the last one of VGG16 are frozen. Only the last block is trained. The accuracy of over 80% is quite good with fewer data (e.g. only about 1000 monkeys). This shows that the patching algorithm works also for complex datasets.
Experiments on engagement is also done for this dataset. Instead of just use fully connected layer, a Conv-block is also applied in the patch network. We can see the improvement of accuracy if the engagement moves towards the end of the network. This shows the same result as for other datasets.
The experiments show that the fine-tuning method archieves better accuracy than engagement. The fine-tuning method has the advantage that it uses the pre-trained weights of VGG16, while the patch network is trained from scratch.

Future work
This thesis mainly focused on network patching algorithm using streaming data. Experiments on engagements are also conducted. Several datasets are applied for our experiments. For the further exploring of patching and engagement we can
1) use more datasets. Besides MNIST, NIST and Dog-Monkey datasets, other datasets can be applied to research more aspects of patching performance and data properties (black-white vs. color, simple and complex images, ...)
2) use more CNN types. Besides VGG16, Resnet, Xception, Inception, MobileNet, etc. are also interesting to be applied to observe probably different behaviour of the patching network
3) find some methods to avoid "fragile co-adaptation". Research on relationship of these co-adaptation and data changed type (flip, appear, ...) is also interesting to explore
4) find more way to optimize the patching networks. Besides adding some dropout and normalization layers and modifying optimizer and learning rate we did, other hyperparameters and methods can be explored
5) build patch network differently. Other than one fully connected hidden layer or one Conv-block plus fully connected layer we used for Dog-Monkey dataset, other structures can be used and experimented on different datasets
6) build a website to receive data for evaluation and training. In this thesis the data stream in batches is a simulation of reality. A website that receives data all the time and evaluate and train the data "live" is more realistic

